window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "needlehaystack", "modulename": "needlehaystack", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators", "modulename": "needlehaystack.evaluators", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.evaluator", "modulename": "needlehaystack.evaluators.evaluator", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.evaluator.Evaluator", "modulename": "needlehaystack.evaluators.evaluator", "qualname": "Evaluator", "kind": "class", "doc": "<p>Helper class that provides a standard way to create an ABC using\ninheritance.</p>\n", "bases": "abc.ABC"}, {"fullname": "needlehaystack.evaluators.evaluator.Evaluator.CRITERIA", "modulename": "needlehaystack.evaluators.evaluator", "qualname": "Evaluator.CRITERIA", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict[str, str]"}, {"fullname": "needlehaystack.evaluators.evaluator.Evaluator.evaluate_response", "modulename": "needlehaystack.evaluators.evaluator", "qualname": "Evaluator.evaluate_response", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">response</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.evaluators.langsmith", "modulename": "needlehaystack.evaluators.langsmith", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.langsmith.score_relevance", "modulename": "needlehaystack.evaluators.langsmith", "qualname": "score_relevance", "kind": "variable", "doc": "<p>A custom evaluator function that grades the language model's response based on its relevance\nto a reference answer.</p>\n\n<p>Args:\n    run (Run): The execution run containing the model's response.\n    example (Union[Example, None]): An optional example containing the reference answer.</p>\n\n<p>Returns:\n    EvaluationResult: The result of the evaluation, containing the relevance score.</p>\n", "default_value": "&lt;DynamicRunEvaluator score_relevance&gt;"}, {"fullname": "needlehaystack.evaluators.langsmith.LangSmithEvaluator", "modulename": "needlehaystack.evaluators.langsmith", "qualname": "LangSmithEvaluator", "kind": "class", "doc": "<p>An evaluator class that leverages the LangSmith API for evaluating language models' performance on specific tasks. \nThis class primarily focuses on evaluating the ability of a language model to retrieve and accurately present information \nfrom a provided context (the \"needle\" in a \"haystack\").</p>\n"}, {"fullname": "needlehaystack.evaluators.langsmith.LangSmithEvaluator.evaluate_chain", "modulename": "needlehaystack.evaluators.langsmith", "qualname": "LangSmithEvaluator.evaluate_chain", "kind": "function", "doc": "<p>Evaluates a language model's chain of operations, specifically focusing on the model's ability to \nretrieve information accurately from a given context. This method defines a custom evaluator that\ngrades the language model's responses based on relevance to a reference answer.</p>\n\n<p>Args:\n    chain: The LangChain runnable or chain of operations to be evaluated.\n    context_length (int): The length of the context in tokens.\n    depth_percent (float): The percentage depth in the context where the information (needle) is located.\n    model_name (str): The name of the language model being evaluated.\n    eval_set (str): The evaluation set identifier, used to categorize and reference the evaluation.\n    num_needles (int): The number of needles in the haystack. \n    needles (list[str]): The needles inserted into the haystack. \n    insertion_percentages (list[float]): The location of each needle in the haystack. </p>\n\n<p>Details:\n    The evaluation involves creating a grading prompt that asks the model to grade student responses\n    based on their relevance to a given reference answer. This approach allows for quantifying the\n    model's accuracy in retrieving and synthesizing information from the provided context.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">chain</span>,</span><span class=\"param\">\t<span class=\"n\">context_length</span>,</span><span class=\"param\">\t<span class=\"n\">depth_percent</span>,</span><span class=\"param\">\t<span class=\"n\">model_name</span>,</span><span class=\"param\">\t<span class=\"n\">eval_set</span>,</span><span class=\"param\">\t<span class=\"n\">num_needles</span>,</span><span class=\"param\">\t<span class=\"n\">needles</span>,</span><span class=\"param\">\t<span class=\"n\">insertion_percentages</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.evaluators.openai", "modulename": "needlehaystack.evaluators.openai", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator", "kind": "class", "doc": "<p>Helper class that provides a standard way to create an ABC using\ninheritance.</p>\n", "bases": "needlehaystack.evaluators.evaluator.Evaluator"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.__init__", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_name</strong>:  The name of the model.</li>\n<li><strong>model_kwargs: Model configuration. Default is {temperature</strong>:  0}</li>\n<li><strong>true_answer</strong>:  The true answer to the question asked.</li>\n<li><strong>question_asked</strong>:  The question asked to the model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;gpt-4.1-mini&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">model_kwargs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;temperature&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}</span>,</span><span class=\"param\">\t<span class=\"n\">true_answer</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">question_asked</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.DEFAULT_MODEL_KWARGS", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.DEFAULT_MODEL_KWARGS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict", "default_value": "{&#x27;temperature&#x27;: 0}"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.CRITERIA", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.CRITERIA", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;accuracy&#x27;: &#x27;\\n                Score 1: The answer is completely unrelated to the reference.\\n                Score 3: The answer has minor relevance but does not align with the reference.\\n                Score 5: The answer has moderate relevance but contains inaccuracies.\\n                Score 7: The answer aligns with the reference but has minor omissions.\\n                Score 10: The answer is completely accurate and aligns perfectly with the reference.\\n                Only respond with a numberical score&#x27;}"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.model_name", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.model_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.model_kwargs", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.model_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.true_answer", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.true_answer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.question_asked", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.question_asked", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.api_key", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.api_key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.client", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.client", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.evaluators.openai.OpenAIEvaluator.evaluate_response", "modulename": "needlehaystack.evaluators.openai", "qualname": "OpenAIEvaluator.evaluate_response", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">response</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester", "kind": "class", "doc": "<p>Extends LLMNeedleHaystackTester to support testing with multiple needles in the haystack.</p>\n\n<p>Attributes:\n    needles (list): A list of needles (facts) to insert into the haystack (context).\n    model_to_test (ModelProvider): The model being tested.\n    evaluator (Evaluator): The evaluator used to assess the model's performance.\n    print_ongoing_status (bool): Flag to print ongoing status messages.\n    eval_set (str): The evaluation set identifier.</p>\n", "bases": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.__init__", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.__init__", "kind": "function", "doc": "<p>:model_to_test: The model to test. Default is None.\n:evaluator: An evaluator to evaluate the model's response. Default is None.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>needle</strong>:  The needle to be found in the haystack. Default is None.</li>\n<li><strong>haystack_dir</strong>:  The directory of text files to use as background context (or a haystack) in which the needle is to be found. Default is Paul Graham Essays.</li>\n<li><strong>retrieval_question</strong>:  The question which with to prompt the model to do the retrieval.</li>\n<li><strong>results_version</strong>:  In case you would like to try the same combination of model, context length, and depth % multiple times, change the results version other than 1</li>\n<li><strong>num_concurrent_requests</strong>:  Due to volume, this object is set up to run concurrent requests, default = 1. Be careful of rate limits.</li>\n<li><strong>save_results: Whether or not you would like to save your contexts to file. Warning</strong>:  These will get long! Default = True</li>\n<li><strong>save_contexts: Whether or not you would like to save your contexts to file. Warning</strong>:  These will get long! Default is True.</li>\n<li><strong>final_context_length_buffer</strong>:  The amount of cushion you'd like to leave off the input context to allow for the output context. Default 200 tokens</li>\n<li><strong>context_lengths_min</strong>:  The minimum length of the context. Default is 1000.</li>\n<li><strong>context_lengths_max</strong>:  The maximum length of the context. Default is 200000.</li>\n<li><strong>context_lengths_num_intervals</strong>:  The number of intervals for the context length. Default is 35.</li>\n<li><strong>context_lengths</strong>:  The lengths of the context. Default is None.</li>\n<li><strong>document_depth_percent_min</strong>:  The minimum depth percent of the document. Default is 0.</li>\n<li><strong>document_depth_percent_max</strong>:  The maximum depth percent of the document. Default is 100.</li>\n<li><strong>document_depth_percent_intervals</strong>:  The number of intervals for the document depth percent. Default is 35.</li>\n<li><strong>document_depth_percents</strong>:  The depth percentages of the document. Default is None.</li>\n<li><strong>document_depth_percent_interval_type</strong>:  The type of interval for the document depth percent. Must be either 'linear' or 'sigmoid'. Default is 'linear'.</li>\n<li><strong>seconds_to_sleep_between_completions</strong>:  The number of seconds to sleep between completions. Default is None.</li>\n<li><strong>print_ongoing_status</strong>:  Whether or not to print the ongoing status. Default is True.</li>\n<li><strong>kwargs</strong>:  Additional arguments.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"o\">*</span><span class=\"n\">args</span>,</span><span class=\"param\">\t<span class=\"n\">needles</span><span class=\"o\">=</span><span class=\"p\">[]</span>,</span><span class=\"param\">\t<span class=\"n\">model_to_test</span><span class=\"p\">:</span> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">providers</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">ModelProvider</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">evaluator</span><span class=\"p\">:</span> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">evaluators</span><span class=\"o\">.</span><span class=\"n\">evaluator</span><span class=\"o\">.</span><span class=\"n\">Evaluator</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">print_ongoing_status</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">eval_set</span><span class=\"o\">=</span><span class=\"s1\">&#39;multi-needle-eval-sf&#39;</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.needles", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.needles", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.evaluator", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.evaluator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.model_to_test", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.model_to_test", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.eval_set", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.eval_set", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.model_name", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.model_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.print_ongoing_status", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.print_ongoing_status", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.insertion_percentages", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.insertion_percentages", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.insert_needles", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.insert_needles", "kind": "function", "doc": "<p>Inserts multiple needles (specific facts or pieces of information) into the original context string at \ndesignated depth percentages, effectively distributing these needles throughout the context. This method \nis designed to test a model's ability to retrieve specific information (needles) from a larger body of text \n(haystack) based on the placement depth of these needles.</p>\n\n<p>The method first encodes the context and each needle into tokens to calculate their lengths in tokens. \nIt then adjusts the context length to accommodate the final buffer length. This is crucial for ensuring \nthat the total token count (context plus needles) does not exceed the maximum allowable context length, \nwhich might otherwise lead to information being truncated.</p>\n\n<p>This approach calculates the initial insertion point for the first needle as before but then calculates even \nspacing for the remaining needles based on the remaining context length. It ensures that needles are \ndistributed as evenly as possible throughout the context after the first insertion. </p>\n\n<p>Args:\n    context (str): The original context string.\n    depth_percent (float): The depth percent at which to insert the needles.\n    context_length (int): The total length of the context in tokens, adjusted for final buffer.</p>\n\n<p>Returns:\n    str: The new context with needles inserted.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span>, </span><span class=\"param\"><span class=\"n\">depth_percent</span>, </span><span class=\"param\"><span class=\"n\">context_length</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.encode_and_trim", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.encode_and_trim", "kind": "function", "doc": "<p>Encodes the context to tokens and trims it to the specified length.</p>\n\n<p>Args:\n    context (str): The context to encode and trim.\n    context_length (int): The desired length of the context in tokens.</p>\n\n<p>Returns:\n    str: The encoded and trimmed context.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span>, </span><span class=\"param\"><span class=\"n\">context_length</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.generate_context", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.generate_context", "kind": "function", "doc": "<p>Generates a context of a specified length and inserts needles at given depth percentages.</p>\n\n<p>Args:\n    context_length (int): The total length of the context in tokens.\n    depth_percent (float): The depth percent for needle insertion.</p>\n\n<p>Returns:\n    str: The context with needles inserted.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context_length</span>, </span><span class=\"param\"><span class=\"n\">depth_percent</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.evaluate_and_log", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.evaluate_and_log", "kind": "function", "doc": "<p>Evaluates the model's performance with the generated context and logs the results.</p>\n\n<p>Args:\n    context_length (int): The length of the context in tokens.\n    depth_percent (float): The depth percent for needle insertion.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context_length</span>, </span><span class=\"param\"><span class=\"n\">depth_percent</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.bound_evaluate_and_log", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.bound_evaluate_and_log", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">sem</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.run_test", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.run_test", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.print_start_test_summary", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.print_start_test_summary", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_multi_needle_haystack_tester.LLMMultiNeedleHaystackTester.start_test", "modulename": "needlehaystack.llm_multi_needle_haystack_tester", "qualname": "LLMMultiNeedleHaystackTester.start_test", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester", "modulename": "needlehaystack.llm_needle_haystack_tester", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester", "kind": "class", "doc": "<p>This class is used to test the LLM Needle Haystack.</p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.__init__", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.__init__", "kind": "function", "doc": "<p>:model_to_test: The model to test. Default is None.\n:evaluator: An evaluator to evaluate the model's response. Default is None.</p>\n\n<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>needle</strong>:  The needle to be found in the haystack. Default is None.</li>\n<li><strong>haystack_dir</strong>:  The directory of text files to use as background context (or a haystack) in which the needle is to be found. Default is Paul Graham Essays.</li>\n<li><strong>retrieval_question</strong>:  The question which with to prompt the model to do the retrieval.</li>\n<li><strong>results_version</strong>:  In case you would like to try the same combination of model, context length, and depth % multiple times, change the results version other than 1</li>\n<li><strong>num_concurrent_requests</strong>:  Due to volume, this object is set up to run concurrent requests, default = 1. Be careful of rate limits.</li>\n<li><strong>save_results: Whether or not you would like to save your contexts to file. Warning</strong>:  These will get long! Default = True</li>\n<li><strong>save_contexts: Whether or not you would like to save your contexts to file. Warning</strong>:  These will get long! Default is True.</li>\n<li><strong>final_context_length_buffer</strong>:  The amount of cushion you'd like to leave off the input context to allow for the output context. Default 200 tokens</li>\n<li><strong>context_lengths_min</strong>:  The minimum length of the context. Default is 1000.</li>\n<li><strong>context_lengths_max</strong>:  The maximum length of the context. Default is 200000.</li>\n<li><strong>context_lengths_num_intervals</strong>:  The number of intervals for the context length. Default is 35.</li>\n<li><strong>context_lengths</strong>:  The lengths of the context. Default is None.</li>\n<li><strong>document_depth_percent_min</strong>:  The minimum depth percent of the document. Default is 0.</li>\n<li><strong>document_depth_percent_max</strong>:  The maximum depth percent of the document. Default is 100.</li>\n<li><strong>document_depth_percent_intervals</strong>:  The number of intervals for the document depth percent. Default is 35.</li>\n<li><strong>document_depth_percents</strong>:  The depth percentages of the document. Default is None.</li>\n<li><strong>document_depth_percent_interval_type</strong>:  The type of interval for the document depth percent. Must be either 'linear' or 'sigmoid'. Default is 'linear'.</li>\n<li><strong>seconds_to_sleep_between_completions</strong>:  The number of seconds to sleep between completions. Default is None.</li>\n<li><strong>print_ongoing_status</strong>:  Whether or not to print the ongoing status. Default is True.</li>\n<li><strong>kwargs</strong>:  Additional arguments.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_to_test</span><span class=\"p\">:</span> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">providers</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">ModelProvider</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">evaluator</span><span class=\"p\">:</span> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">evaluators</span><span class=\"o\">.</span><span class=\"n\">evaluator</span><span class=\"o\">.</span><span class=\"n\">Evaluator</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">needle</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">haystack_dir</span><span class=\"o\">=</span><span class=\"s1\">&#39;PaulGrahamEssays&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_question</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">results_version</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths_min</span><span class=\"o\">=</span><span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths_max</span><span class=\"o\">=</span><span class=\"mi\">16000</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths_num_intervals</span><span class=\"o\">=</span><span class=\"mi\">35</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_min</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_max</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_intervals</span><span class=\"o\">=</span><span class=\"mi\">35</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percents</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_interval_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;linear&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_concurrent_requests</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">save_results</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">save_contexts</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">final_context_length_buffer</span><span class=\"o\">=</span><span class=\"mi\">200</span>,</span><span class=\"param\">\t<span class=\"n\">seconds_to_sleep_between_completions</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">print_ongoing_status</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.needle", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.needle", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.haystack_dir", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.haystack_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.retrieval_question", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.retrieval_question", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.results_version", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.results_version", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.num_concurrent_requests", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.num_concurrent_requests", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.save_results", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.save_results", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.final_context_length_buffer", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.final_context_length_buffer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.save_contexts", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.save_contexts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.seconds_to_sleep_between_completions", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.seconds_to_sleep_between_completions", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.print_ongoing_status", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.print_ongoing_status", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.testing_results", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.testing_results", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.model_to_test", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.model_to_test", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.model_name", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.model_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.evaluation_model", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.evaluation_model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.logistic", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.logistic", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">100</span>, </span><span class=\"param\"><span class=\"n\">x0</span><span class=\"o\">=</span><span class=\"mi\">50</span>, </span><span class=\"param\"><span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mf\">0.1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.sigmoid", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.sigmoid", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.bound_evaluate_and_log", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.bound_evaluate_and_log", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">sem</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.run_test", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.run_test", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.evaluate_and_log", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.evaluate_and_log", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context_length</span>, </span><span class=\"param\"><span class=\"n\">depth_percent</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.result_exists", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.result_exists", "kind": "function", "doc": "<p>Checks to see if a result has already been evaluated or not</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context_length</span>, </span><span class=\"param\"><span class=\"n\">depth_percent</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.generate_context", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.generate_context", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context_length</span>, </span><span class=\"param\"><span class=\"n\">depth_percent</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.insert_needle", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.insert_needle", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span>, </span><span class=\"param\"><span class=\"n\">depth_percent</span>, </span><span class=\"param\"><span class=\"n\">context_length</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.get_context_length_in_tokens", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.get_context_length_in_tokens", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.read_context_files", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.read_context_files", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.encode_and_trim", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.encode_and_trim", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span>, </span><span class=\"param\"><span class=\"n\">context_length</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.get_results", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.get_results", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.print_start_test_summary", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.print_start_test_summary", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.llm_needle_haystack_tester.LLMNeedleHaystackTester.start_test", "modulename": "needlehaystack.llm_needle_haystack_tester", "qualname": "LLMNeedleHaystackTester.start_test", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers", "modulename": "needlehaystack.providers", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.anthropic", "modulename": "needlehaystack.providers.anthropic", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic", "kind": "class", "doc": "<p>Helper class that provides a standard way to create an ABC using\ninheritance.</p>\n", "bases": "needlehaystack.providers.model.ModelProvider"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.__init__", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_name</strong>:  The name of the model. Default is 'claude'.</li>\n<li><strong>model_kwargs: Model configuration. Default is {max_tokens_to_sample: 300, temperature</strong>:  0}</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;claude-2.1&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">model_kwargs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;max_tokens_to_sample&#39;</span><span class=\"p\">:</span> <span class=\"mi\">300</span><span class=\"p\">,</span> <span class=\"s1\">&#39;temperature&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}</span></span>)</span>"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.DEFAULT_MODEL_KWARGS", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.DEFAULT_MODEL_KWARGS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict", "default_value": "{&#x27;max_tokens_to_sample&#x27;: 300, &#x27;temperature&#x27;: 0}"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.model_name", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.model_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.model_kwargs", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.model_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.api_key", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.api_key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.model", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.tokenizer", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.tokenizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.evaluate_model", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.evaluate_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.generate_prompt", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.generate_prompt", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_question</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.encode_text_to_tokens", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.encode_text_to_tokens", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.decode_tokens", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.decode_tokens", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tokens</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">context_length</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.anthropic.Anthropic.get_langchain_runnable", "modulename": "needlehaystack.providers.anthropic", "qualname": "Anthropic.get_langchain_runnable", "kind": "function", "doc": "<p>Creates a LangChain runnable that constructs a prompt based on a given context and a question, \nqueries the Anthropic model, and returns the model's response. This method leverages the LangChain \nlibrary to build a sequence of operations: extracting input variables, generating a prompt, \nquerying the model, and processing the response.</p>\n\n<p>Args:\n    context (str): The context or background information relevant to the user's question. \n    This context is provided to the model to aid in generating relevant and accurate responses.</p>\n\n<p>Returns:\n    str: A LangChain runnable object that can be executed to obtain the model's response to a \n    dynamically provided question. The runnable encapsulates the entire process from prompt \n    generation to response retrieval.</p>\n\n<p>Example:\n    To use the runnable:\n        - Define the context and question.\n        - Execute the runnable with these parameters to get the model's response.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.cohere", "modulename": "needlehaystack.providers.cohere", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.cohere.Cohere", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere", "kind": "class", "doc": "<p>Helper class that provides a standard way to create an ABC using\ninheritance.</p>\n", "bases": "needlehaystack.providers.model.ModelProvider"}, {"fullname": "needlehaystack.providers.cohere.Cohere.__init__", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.__init__", "kind": "function", "doc": "<h6 id=\"parameters\">Parameters</h6>\n\n<ul>\n<li><strong>model_name</strong>:  The name of the model. Default is 'command-r'.</li>\n<li><strong>model_kwargs: Model configuration. Default is {max_tokens_to_sample: 300, temperature</strong>:  0}</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;command-r&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">model_kwargs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;max_tokens&#39;</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"s1\">&#39;temperature&#39;</span><span class=\"p\">:</span> <span class=\"mf\">0.3</span><span class=\"p\">}</span></span>)</span>"}, {"fullname": "needlehaystack.providers.cohere.Cohere.DEFAULT_MODEL_KWARGS", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.DEFAULT_MODEL_KWARGS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict", "default_value": "{&#x27;max_tokens&#x27;: 50, &#x27;temperature&#x27;: 0.3}"}, {"fullname": "needlehaystack.providers.cohere.Cohere.model_name", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.model_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.cohere.Cohere.model_kwargs", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.model_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.cohere.Cohere.api_key", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.api_key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.cohere.Cohere.client", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.client", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.cohere.Cohere.evaluate_model", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.evaluate_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.providers.cohere.Cohere.generate_prompt", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.generate_prompt", "kind": "function", "doc": "<p>Prepares a chat-formatted prompt\nArgs:\n    context (str): The needle in a haystack context\n    retrieval_question (str): The needle retrieval question</p>\n\n<p>Returns:\n    tuple[str, list[dict[str, str]]]: prompt encoded as last message, and chat history</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_question</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.cohere.Cohere.encode_text_to_tokens", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.encode_text_to_tokens", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.cohere.Cohere.decode_tokens", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.decode_tokens", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tokens</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">context_length</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.cohere.Cohere.get_langchain_runnable", "modulename": "needlehaystack.providers.cohere", "qualname": "Cohere.get_langchain_runnable", "kind": "function", "doc": "<p>Creates a LangChain runnable that constructs a prompt based on a given context and a question.</p>\n\n<p>Args:\n    context (str): The context or background information relevant to the user's question. \n    This context is provided to the model to aid in generating relevant and accurate responses.</p>\n\n<p>Returns:\n    str: A LangChain runnable object that can be executed to obtain the model's response to a \n    dynamically provided question. The runnable encapsulates the entire process from prompt \n    generation to response retrieval.</p>\n\n<p>Example:\n    To use the runnable:\n        - Define the context and question.\n        - Execute the runnable with these parameters to get the model's response.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.model", "modulename": "needlehaystack.providers.model", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.model.ModelProvider", "modulename": "needlehaystack.providers.model", "qualname": "ModelProvider", "kind": "class", "doc": "<p>Helper class that provides a standard way to create an ABC using\ninheritance.</p>\n", "bases": "abc.ABC"}, {"fullname": "needlehaystack.providers.model.ModelProvider.evaluate_model", "modulename": "needlehaystack.providers.model", "qualname": "ModelProvider.evaluate_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.providers.model.ModelProvider.generate_prompt", "modulename": "needlehaystack.providers.model", "qualname": "ModelProvider.generate_prompt", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_question</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.model.ModelProvider.encode_text_to_tokens", "modulename": "needlehaystack.providers.model", "qualname": "ModelProvider.encode_text_to_tokens", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.model.ModelProvider.decode_tokens", "modulename": "needlehaystack.providers.model", "qualname": "ModelProvider.decode_tokens", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tokens</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">context_length</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.openai", "modulename": "needlehaystack.providers.openai", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.openai.OpenAI", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI", "kind": "class", "doc": "<p>A wrapper class for interacting with OpenAI's API, providing methods to encode text, generate prompts,\nevaluate models, and create LangChain runnables for language model interactions.</p>\n\n<p>Attributes:\n    model_name (str): The name of the OpenAI model to use for evaluations and interactions.\n    model (AsyncOpenAI): An instance of the AsyncOpenAI client for asynchronous API calls.\n    tokenizer: A tokenizer instance for encoding and decoding text to and from token representations.</p>\n", "bases": "needlehaystack.providers.model.ModelProvider"}, {"fullname": "needlehaystack.providers.openai.OpenAI.__init__", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.__init__", "kind": "function", "doc": "<p>Initializes the OpenAI model provider with a specific model.</p>\n\n<p>Args:\n    model_name (str): The name of the OpenAI model to use. Defaults to 'gpt-4.1-mini'.\n    model_kwargs (dict): Model configuration. Defaults to {max_tokens: 300, temperature: 0}.</p>\n\n<p>Raises:\n    ValueError: If NIAH_MODEL_API_KEY is not found in the environment.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;gpt-4.1-mini&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">model_kwargs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;max_tokens&#39;</span><span class=\"p\">:</span> <span class=\"mi\">300</span><span class=\"p\">,</span> <span class=\"s1\">&#39;temperature&#39;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}</span></span>)</span>"}, {"fullname": "needlehaystack.providers.openai.OpenAI.DEFAULT_MODEL_KWARGS", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.DEFAULT_MODEL_KWARGS", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict", "default_value": "{&#x27;max_tokens&#x27;: 300, &#x27;temperature&#x27;: 0}"}, {"fullname": "needlehaystack.providers.openai.OpenAI.model_name", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.model_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.openai.OpenAI.model_kwargs", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.model_kwargs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.openai.OpenAI.api_key", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.api_key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.openai.OpenAI.model", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.openai.OpenAI.tokenizer", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.tokenizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.providers.openai.OpenAI.evaluate_model", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.evaluate_model", "kind": "function", "doc": "<p>Evaluates a given prompt using the OpenAI model and retrieves the model's response.</p>\n\n<p>Args:\n    prompt (str): The prompt to send to the model.</p>\n\n<p>Returns:\n    str: The content of the model's response to the prompt.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "async def"}, {"fullname": "needlehaystack.providers.openai.OpenAI.generate_prompt", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.generate_prompt", "kind": "function", "doc": "<p>Generates a structured prompt for querying the model, based on a given context and retrieval question.</p>\n\n<p>Args:\n    context (str): The context or background information relevant to the question.\n    retrieval_question (str): The specific question to be answered by the model.</p>\n\n<p>Returns:\n    list[dict[str, str]]: A list of dictionaries representing the structured prompt, including roles and content for system and user messages.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_question</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.openai.OpenAI.encode_text_to_tokens", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.encode_text_to_tokens", "kind": "function", "doc": "<p>Encodes a given text string to a sequence of tokens using the model's tokenizer.</p>\n\n<p>Args:\n    text (str): The text to encode.</p>\n\n<p>Returns:\n    list[int]: A list of token IDs representing the encoded text.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.openai.OpenAI.decode_tokens", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.decode_tokens", "kind": "function", "doc": "<p>Decodes a sequence of tokens back into a text string using the model's tokenizer.</p>\n\n<p>Args:\n    tokens (list[int]): The sequence of token IDs to decode.\n    context_length (Optional[int], optional): An optional length specifying the number of tokens to decode. If not provided, decodes all tokens.</p>\n\n<p>Returns:\n    str: The decoded text string.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">tokens</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">context_length</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.providers.openai.OpenAI.get_langchain_runnable", "modulename": "needlehaystack.providers.openai", "qualname": "OpenAI.get_langchain_runnable", "kind": "function", "doc": "<p>Creates a LangChain runnable that constructs a prompt based on a given context and a question, \nqueries the OpenAI model, and returns the model's response. This method leverages the LangChain \nlibrary to build a sequence of operations: extracting input variables, generating a prompt, \nquerying the model, and processing the response.</p>\n\n<p>Args:\n    context (str): The context or background information relevant to the user's question. \n    This context is provided to the model to aid in generating relevant and accurate responses.</p>\n\n<p>Returns:\n    str: A LangChain runnable object that can be executed to obtain the model's response to a \n    dynamically provided question. The runnable encapsulates the entire process from prompt \n    generation to response retrieval.</p>\n\n<p>Example:\n    To use the runnable:\n        - Define the context and question.\n        - Execute the runnable with these parameters to get the model's response.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.run", "modulename": "needlehaystack.run", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "needlehaystack.run.CommandArgs", "modulename": "needlehaystack.run", "qualname": "CommandArgs", "kind": "class", "doc": "<p>Command line arguments for LLM Needle in Haystack testing.</p>\n\n<p>LLMNeedleHaystackTester parameters:</p>\n\n<ul>\n<li>model_to_test: The model to run the needle in a haystack test on. Default is None.</li>\n<li>evaluator: An evaluator to evaluate the model's response. Default is None.</li>\n<li>needle: The statement or fact which will be placed in your context ('haystack')</li>\n<li>haystack_dir: The directory which contains the text files to load as background context. Only text files are supported</li>\n<li>retrieval_question: The question with which to retrieve your needle in the background context</li>\n<li>results_version: You may want to run your test multiple times for the same combination of length/depth, change the version number if so</li>\n<li>num_concurrent_requests: Default: 1. Set higher if you'd like to run more requests in parallel. Keep in mind rate limits.</li>\n<li>save_results: Whether or not you'd like to save your results to file. They will be temporarily saved in the object regardless. True/False. If save_results = True, then this script will populate a result/ directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.</li>\n<li>save_contexts: Whether or not you'd like to save your contexts to file. Warning these will get very long. True/False</li>\n<li>final_context_length_buffer: The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.</li>\n<li>context_lengths_min: The starting point of your context lengths list to iterate</li>\n<li>context_lengths_max: The ending point of your context lengths list to iterate</li>\n<li>context_lengths_num_intervals: The number of intervals between your min/max to iterate through</li>\n<li>context_lengths: A custom set of context lengths. This will override the values set for context_lengths_min, max, and intervals if set</li>\n<li>document_depth_percent_min: The starting point of your document depths. Should be int &gt; 0</li>\n<li>document_depth_percent_max: The ending point of your document depths. Should be int &lt; 100</li>\n<li>document_depth_percent_intervals: The number of iterations to do between your min/max points</li>\n<li>document_depth_percents: A custom set of document depths lengths. This will override the values set for document_depth_percent_min, max, and intervals if set</li>\n<li>document_depth_percent_interval_type: Determines the distribution of depths to iterate over. 'linear' or 'sigmoid'</li>\n<li>seconds_to_sleep_between_completions: Default: None, set # of seconds if you'd like to slow down your requests</li>\n<li>print_ongoing_status: Default: True, whether or not to print the status of test as they complete</li>\n</ul>\n\n<p>LLMMultiNeedleHaystackTester parameters:</p>\n\n<ul>\n<li>multi_needle: True or False, whether to run multi-needle</li>\n<li>needles: List of needles to insert in the context</li>\n</ul>\n\n<p>Other Parameters:</p>\n\n<ul>\n<li>model_name: The name of the model you'd like to use. Should match the exact value which needs to be passed to the api. Ex: For OpenAI inference and evaluator models it would be gpt-4.1-mini.</li>\n</ul>\n"}, {"fullname": "needlehaystack.run.CommandArgs.__init__", "modulename": "needlehaystack.run", "qualname": "CommandArgs.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">provider</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;openai&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">evaluator</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;openai&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;gpt-4.1-mini&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">evaluator_model_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;gpt-4.1-mini&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">needle</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;</span><span class=\"se\">\\n</span><span class=\"s1\">The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">haystack_dir</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;PaulGrahamEssays&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">retrieval_question</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;What is the best thing to do in San Francisco?&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">results_version</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths_min</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths_max</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">16000</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths_num_intervals</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">35</span>,</span><span class=\"param\">\t<span class=\"n\">context_lengths</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_min</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_max</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_intervals</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">35</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percents</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">document_depth_percent_interval_type</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;linear&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">num_concurrent_requests</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">save_results</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">save_contexts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">final_context_length_buffer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">200</span>,</span><span class=\"param\">\t<span class=\"n\">seconds_to_sleep_between_completions</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">print_ongoing_status</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">eval_set</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;multi-needle-eval-pizza-3&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">multi_needle</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">needles</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">factory</span><span class=\"o\">&gt;</span></span>)</span>"}, {"fullname": "needlehaystack.run.CommandArgs.provider", "modulename": "needlehaystack.run", "qualname": "CommandArgs.provider", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;openai&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.evaluator", "modulename": "needlehaystack.run", "qualname": "CommandArgs.evaluator", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;openai&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.model_name", "modulename": "needlehaystack.run", "qualname": "CommandArgs.model_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;gpt-4.1-mini&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.evaluator_model_name", "modulename": "needlehaystack.run", "qualname": "CommandArgs.evaluator_model_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;gpt-4.1-mini&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.needle", "modulename": "needlehaystack.run", "qualname": "CommandArgs.needle", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;\\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.haystack_dir", "modulename": "needlehaystack.run", "qualname": "CommandArgs.haystack_dir", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;PaulGrahamEssays&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.retrieval_question", "modulename": "needlehaystack.run", "qualname": "CommandArgs.retrieval_question", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;What is the best thing to do in San Francisco?&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.results_version", "modulename": "needlehaystack.run", "qualname": "CommandArgs.results_version", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "1"}, {"fullname": "needlehaystack.run.CommandArgs.context_lengths_min", "modulename": "needlehaystack.run", "qualname": "CommandArgs.context_lengths_min", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "1000"}, {"fullname": "needlehaystack.run.CommandArgs.context_lengths_max", "modulename": "needlehaystack.run", "qualname": "CommandArgs.context_lengths_max", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "16000"}, {"fullname": "needlehaystack.run.CommandArgs.context_lengths_num_intervals", "modulename": "needlehaystack.run", "qualname": "CommandArgs.context_lengths_num_intervals", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "35"}, {"fullname": "needlehaystack.run.CommandArgs.context_lengths", "modulename": "needlehaystack.run", "qualname": "CommandArgs.context_lengths", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[list[int]]", "default_value": "None"}, {"fullname": "needlehaystack.run.CommandArgs.document_depth_percent_min", "modulename": "needlehaystack.run", "qualname": "CommandArgs.document_depth_percent_min", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "0"}, {"fullname": "needlehaystack.run.CommandArgs.document_depth_percent_max", "modulename": "needlehaystack.run", "qualname": "CommandArgs.document_depth_percent_max", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "100"}, {"fullname": "needlehaystack.run.CommandArgs.document_depth_percent_intervals", "modulename": "needlehaystack.run", "qualname": "CommandArgs.document_depth_percent_intervals", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "35"}, {"fullname": "needlehaystack.run.CommandArgs.document_depth_percents", "modulename": "needlehaystack.run", "qualname": "CommandArgs.document_depth_percents", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[list[int]]", "default_value": "None"}, {"fullname": "needlehaystack.run.CommandArgs.document_depth_percent_interval_type", "modulename": "needlehaystack.run", "qualname": "CommandArgs.document_depth_percent_interval_type", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;linear&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.num_concurrent_requests", "modulename": "needlehaystack.run", "qualname": "CommandArgs.num_concurrent_requests", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "1"}, {"fullname": "needlehaystack.run.CommandArgs.save_results", "modulename": "needlehaystack.run", "qualname": "CommandArgs.save_results", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[bool]", "default_value": "True"}, {"fullname": "needlehaystack.run.CommandArgs.save_contexts", "modulename": "needlehaystack.run", "qualname": "CommandArgs.save_contexts", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[bool]", "default_value": "True"}, {"fullname": "needlehaystack.run.CommandArgs.final_context_length_buffer", "modulename": "needlehaystack.run", "qualname": "CommandArgs.final_context_length_buffer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": "200"}, {"fullname": "needlehaystack.run.CommandArgs.seconds_to_sleep_between_completions", "modulename": "needlehaystack.run", "qualname": "CommandArgs.seconds_to_sleep_between_completions", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[float]", "default_value": "None"}, {"fullname": "needlehaystack.run.CommandArgs.print_ongoing_status", "modulename": "needlehaystack.run", "qualname": "CommandArgs.print_ongoing_status", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[bool]", "default_value": "True"}, {"fullname": "needlehaystack.run.CommandArgs.eval_set", "modulename": "needlehaystack.run", "qualname": "CommandArgs.eval_set", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": "&#x27;multi-needle-eval-pizza-3&#x27;"}, {"fullname": "needlehaystack.run.CommandArgs.multi_needle", "modulename": "needlehaystack.run", "qualname": "CommandArgs.multi_needle", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[bool]", "default_value": "False"}, {"fullname": "needlehaystack.run.CommandArgs.needles", "modulename": "needlehaystack.run", "qualname": "CommandArgs.needles", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list[str]"}, {"fullname": "needlehaystack.run.get_model_to_test", "modulename": "needlehaystack.run", "qualname": "get_model_to_test", "kind": "function", "doc": "<p>Determines and returns the appropriate model provider based on the provided command arguments.</p>\n\n<p>Args:\n    args (CommandArgs): The command line arguments parsed into a CommandArgs dataclass instance.</p>\n\n<p>Returns:\n    ModelProvider: An instance of the specified model provider class.</p>\n\n<p>Raises:\n    ValueError: If the specified provider is not supported.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"o\">.</span><span class=\"n\">CommandArgs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">providers</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">ModelProvider</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.run.get_evaluator", "modulename": "needlehaystack.run", "qualname": "get_evaluator", "kind": "function", "doc": "<p>Selects and returns the appropriate evaluator based on the provided command arguments.</p>\n\n<p>Args:\n    args (CommandArgs): The command line arguments parsed into a CommandArgs dataclass instance.</p>\n\n<p>Returns:\n    Evaluator: An instance of the specified evaluator class.</p>\n\n<p>Raises:\n    ValueError: If the specified evaluator is not supported.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"o\">.</span><span class=\"n\">CommandArgs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">needlehaystack</span><span class=\"o\">.</span><span class=\"n\">evaluators</span><span class=\"o\">.</span><span class=\"n\">evaluator</span><span class=\"o\">.</span><span class=\"n\">Evaluator</span>:</span></span>", "funcdef": "def"}, {"fullname": "needlehaystack.run.main", "modulename": "needlehaystack.run", "qualname": "main", "kind": "function", "doc": "<p>The main function to execute the testing process based on command line arguments.</p>\n\n<p>It parses the command line arguments, selects the appropriate model provider and evaluator,\nand initiates the testing process either for single-needle or multi-needle scenarios.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();